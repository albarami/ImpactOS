"""BoQ structuring pipeline — MVP-2 Section 8.4.

Converts extracted tables into normalized BoQLineItem records.
This is a deterministic pipeline — no LLM calls.

Steps per spec:
1. Column normalization — map variant headers to canonical fields
2. Value parsing — parse numeric formats, currencies, units
3. Header deduplication — remove repeated headers from multi-page tables
4. Section/subtotal filtering — skip section headers and subtotal rows
5. Completeness scoring — fraction of expected fields populated
6. Line item creation — persist BoQLineItem linked to EvidenceSnippets
"""

import re
from uuid import UUID

from src.models.document import (
    BoQLineItem,
    DocumentGraph,
    ExtractedTable,
    TableCell,
)
from src.models.governance import EvidenceSnippet


# ---------------------------------------------------------------------------
# Header normalization mapping
# ---------------------------------------------------------------------------

# Maps various header synonyms to canonical field names.
_HEADER_ALIASES: dict[str, str] = {
    # description
    "description": "description",
    "item description": "description",
    "item": "description",
    "item name": "description",
    "desc": "description",
    # quantity
    "quantity": "quantity",
    "qty": "quantity",
    "qty.": "quantity",
    # unit
    "unit": "unit",
    "uom": "unit",
    "unit of measure": "unit",
    "units": "unit",
    # unit_price
    "unit price": "unit_price",
    "unit_price": "unit_price",
    "rate": "unit_price",
    "price": "unit_price",
    # total_value
    "total": "total_value",
    "total_value": "total_value",
    "total price": "total_value",
    "total value": "total_value",
    "total amount": "total_value",
    "amount": "total_value",
}

# Canonical fields used for completeness scoring
_SCORED_FIELDS = ("description", "quantity", "unit", "unit_price", "total_value")

# Patterns to strip from header text for matching
_HEADER_STRIP_RE = re.compile(r"\s*\([^)]*\)\s*")  # removes parentheticals like "(SAR)"
_SUBTOTAL_RE = re.compile(r"(?i)^\s*(sub\s*total|total|grand\s*total)\s*$")


class BoQStructuringPipeline:
    """Deterministic BoQ structuring: tables → normalized BoQLineItem list."""

    def structure(
        self,
        *,
        document_graph: DocumentGraph,
        evidence_snippets: list[EvidenceSnippet],
        extraction_job_id: UUID,
    ) -> list[BoQLineItem]:
        """Run the full structuring pipeline on a DocumentGraph.

        Args:
            document_graph: Extracted tables from ExtractionService.
            evidence_snippets: Snippets generated by ExtractionService
                (one per data row, keyed by table_id + row).
            extraction_job_id: ID of the extraction job for provenance.

        Returns:
            List of normalized BoQLineItem records, each linked to evidence.
        """
        items: list[BoQLineItem] = []

        # Build snippet lookup: (table_id, row) -> EvidenceSnippet
        snippet_lookup: dict[tuple[str, int], EvidenceSnippet] = {}
        for snippet in evidence_snippets:
            if snippet.table_cell_ref is not None:
                key = (snippet.table_cell_ref.table_id, snippet.table_cell_ref.row)
                snippet_lookup[key] = snippet

        for page in document_graph.pages:
            for table in page.tables:
                page_items = self._process_table(
                    table=table,
                    doc_id=document_graph.document_id,
                    extraction_job_id=extraction_job_id,
                    page_number=page.page_number,
                    snippet_lookup=snippet_lookup,
                )
                items.extend(page_items)

        return items

    def _process_table(
        self,
        *,
        table: ExtractedTable,
        doc_id: UUID,
        extraction_job_id: UUID,
        page_number: int,
        snippet_lookup: dict[tuple[str, int], EvidenceSnippet],
    ) -> list[BoQLineItem]:
        """Process a single extracted table into BoQLineItem records."""
        # Group cells by row
        rows_map: dict[int, list[TableCell]] = {}
        for cell in table.cells:
            rows_map.setdefault(cell.row, []).append(cell)

        if not rows_map:
            return []

        # Row 0 is the header; normalize it
        sorted_rows = sorted(rows_map.keys())
        header_row = sorted_rows[0]
        header_cells = sorted(rows_map[header_row], key=lambda c: c.col)
        col_map = self._normalize_headers(header_cells)

        # Collect header texts for dedup detection
        header_texts = tuple(c.text.strip().lower() for c in header_cells)

        items: list[BoQLineItem] = []
        for row_idx in sorted_rows[1:]:
            row_cells = sorted(rows_map[row_idx], key=lambda c: c.col)

            # Dedup: skip rows that look like repeated headers
            row_texts = tuple(c.text.strip().lower() for c in row_cells)
            if row_texts == header_texts:
                continue

            # Parse the row into field values
            parsed = self._parse_row(row_cells, col_map)

            # Skip section headers (all numeric fields empty)
            if self._is_section_header(parsed):
                continue

            # Skip subtotal rows
            if self._is_subtotal(parsed):
                continue

            # Completeness score
            completeness = self._compute_completeness(parsed)

            # Find evidence snippet for this row
            snippet_key = (table.table_id, row_idx)
            snippet = snippet_lookup.get(snippet_key)
            evidence_ids = [snippet.snippet_id] if snippet else []

            # If no snippet found, we still need at least one evidence ID
            # This shouldn't happen in normal flow but safeguards the constraint
            if not evidence_ids:
                continue

            raw_text = " | ".join(c.text for c in row_cells)

            item = BoQLineItem(
                doc_id=doc_id,
                extraction_job_id=extraction_job_id,
                raw_text=raw_text,
                description=parsed.get("description", ""),
                quantity=parsed.get("quantity"),
                unit=parsed.get("unit"),
                unit_price=parsed.get("unit_price"),
                total_value=parsed.get("total_value"),
                currency_code="SAR",
                page_ref=page_number,
                evidence_snippet_ids=evidence_ids,
                completeness_score=completeness,
            )
            items.append(item)

        return items

    # ------------------------------------------------------------------
    # Step 1: Column normalization
    # ------------------------------------------------------------------

    def _normalize_headers(
        self, header_cells: list[TableCell]
    ) -> dict[int, str]:
        """Map column indices to canonical field names.

        Returns:
            Dict of {col_index: canonical_field_name}.
        """
        col_map: dict[int, str] = {}
        for cell in header_cells:
            raw = cell.text.strip()
            # Strip parenthetical suffixes like "(SAR)"
            cleaned = _HEADER_STRIP_RE.sub("", raw).strip().lower()
            canonical = _HEADER_ALIASES.get(cleaned)
            if canonical is not None:
                col_map[cell.col] = canonical
        return col_map

    # ------------------------------------------------------------------
    # Step 2: Value parsing
    # ------------------------------------------------------------------

    def _parse_row(
        self,
        row_cells: list[TableCell],
        col_map: dict[int, str],
    ) -> dict[str, object]:
        """Parse a data row into a dict of canonical field values."""
        parsed: dict[str, object] = {}
        for cell in row_cells:
            field = col_map.get(cell.col)
            if field is None:
                continue
            text = cell.text.strip()
            if field in ("quantity", "unit_price", "total_value"):
                parsed[field] = self._parse_number(text)
            elif field == "unit":
                parsed[field] = text if text else None
            else:
                parsed[field] = text
        return parsed

    @staticmethod
    def _parse_number(text: str) -> float | None:
        """Parse a numeric string, stripping commas and currency symbols."""
        if not text:
            return None
        # Remove common non-numeric characters (commas, currency symbols)
        cleaned = text.replace(",", "").replace("SAR", "").replace("$", "").strip()
        try:
            return float(cleaned)
        except ValueError:
            return None

    # ------------------------------------------------------------------
    # Step 3 & 4: Filtering (section headers, subtotals)
    # ------------------------------------------------------------------

    @staticmethod
    def _is_section_header(parsed: dict[str, object]) -> bool:
        """A section header row has a description but no numeric values."""
        has_description = bool(parsed.get("description"))
        has_any_number = any(
            parsed.get(f) is not None
            for f in ("quantity", "unit_price", "total_value")
        )
        return has_description and not has_any_number

    @staticmethod
    def _is_subtotal(parsed: dict[str, object]) -> bool:
        """A subtotal row matches subtotal/total patterns."""
        desc = str(parsed.get("description", ""))
        return bool(_SUBTOTAL_RE.match(desc))

    # ------------------------------------------------------------------
    # Step 5: Completeness scoring
    # ------------------------------------------------------------------

    @staticmethod
    def _compute_completeness(parsed: dict[str, object]) -> float:
        """Fraction of _SCORED_FIELDS that have a non-None, non-empty value."""
        filled = 0
        for field in _SCORED_FIELDS:
            val = parsed.get(field)
            if val is not None and val != "":
                filled += 1
        return filled / len(_SCORED_FIELDS)
