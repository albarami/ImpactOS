"""Quality scoring engine — all 7 dimensions (MVP-13, Tasks 2-3).

Deterministic scoring functions that produce DimensionAssessment values
with full provenance (inputs_used, rules_triggered) for every dimension.

Deterministic — no LLM calls.
"""

from __future__ import annotations

from src.quality.config import QualityScoringConfig
from src.quality.models import (
    FREQUENCY_DAYS,
    DimensionAssessment,
    QualityDimension,
    QualityGrade,
    QualitySeverity,
    QualityWarning,
    RunQualityAssessment,
    SourceAge,
    SourceUpdateFrequency,
)

# NOTE: QualitySeverity and QualityWarning are still imported for use by
# composite_score (warning aggregation) and type references in tests.

# Confidence weights for constraints dimension.
_CONSTRAINT_WEIGHTS: dict[str, float] = {
    "HARD": 1.0,
    "ESTIMATED": 0.6,
    "ASSUMED": 0.3,
}

# Workforce confidence label -> score mapping.
_WORKFORCE_SCORES: dict[str, float] = {
    "HIGH": 1.0,
    "MEDIUM": 0.6,
    "LOW": 0.3,
}


class QualityScorer:
    """Scores all 7 quality dimensions with configurable thresholds.

    Each scoring method returns a DimensionAssessment with:
    - score (0.0-1.0)
    - applicable flag
    - inputs_used (provenance: what data was fed in)
    - rules_triggered (provenance: which scoring rules fired)
    - warnings (materiality or quality warnings)
    """

    def __init__(self, config: QualityScoringConfig | None = None) -> None:
        self._config = config or QualityScoringConfig()

    # ---------------------------------------------------------------
    # Dimension 1: Vintage
    # ---------------------------------------------------------------

    def score_vintage(
        self,
        base_year: int,
        current_year: int,
    ) -> DimensionAssessment:
        """Score the vintage (age) of the I-O table.

        Walks config.vintage_thresholds to find the first bracket
        where age <= max_age and returns the corresponding score.
        """
        age = current_year - base_year

        score = 0.0
        rule = "vintage_fallback"
        for max_age, bracket_score in self._config.vintage_thresholds:
            if age <= max_age:
                score = bracket_score
                rule = f"vintage_decay_0_{max_age}yr"
                break

        return DimensionAssessment(
            dimension=QualityDimension.VINTAGE,
            score=score,
            applicable=True,
            inputs_used={
                "base_year": base_year,
                "current_year": current_year,
                "age_years": age,
            },
            rules_triggered=[rule],
        )

    # ---------------------------------------------------------------
    # Dimension 2: Mapping
    # ---------------------------------------------------------------

    def score_mapping(
        self,
        coverage_pct: float,
        confidence_dist: dict[str, float],
        residual_pct: float,
        unresolved_pct: float,
        unresolved_spend_pct: float,
    ) -> DimensionAssessment:
        """Score the sector mapping quality.

        Weighted formula:
            0.4 * coverage + 0.3 * HIGH_confidence + 0.2 * (1-residual) + 0.1 * (1-unresolved)

        Warning generation is handled exclusively by the WarningEngine;
        this method only records which materiality rules were triggered.
        """
        high_conf = confidence_dist.get("HIGH", 0.0)
        score = (
            0.4 * coverage_pct
            + 0.3 * high_conf
            + 0.2 * (1.0 - residual_pct)
            + 0.1 * (1.0 - unresolved_pct)
        )
        # Clamp to [0, 1] for safety.
        score = max(0.0, min(1.0, score))

        rules: list[str] = ["mapping_weighted_score"]

        # Record which materiality rules fired (warnings generated by WarningEngine).
        if unresolved_spend_pct > self._config.mapping_spend_waiver_pct:
            rules.append("materiality_waiver_required")
        elif unresolved_spend_pct > self._config.mapping_spend_critical_pct:
            rules.append("materiality_critical")
        elif unresolved_spend_pct > 0.0:
            rules.append("materiality_warning")

        return DimensionAssessment(
            dimension=QualityDimension.MAPPING,
            score=score,
            applicable=True,
            inputs_used={
                "coverage_pct": coverage_pct,
                "confidence_dist": confidence_dist,
                "residual_pct": residual_pct,
                "unresolved_pct": unresolved_pct,
                "unresolved_spend_pct": unresolved_spend_pct,
            },
            rules_triggered=rules,
        )

    # ---------------------------------------------------------------
    # Dimension 3: Assumptions
    # ---------------------------------------------------------------

    def score_assumptions(
        self,
        ranges_coverage_pct: float,
        approval_rate: float,
    ) -> DimensionAssessment:
        """Score assumption quality from coverage and approval rate.

        Score = 0.5 * ranges_coverage_pct + 0.5 * approval_rate
        """
        score = 0.5 * ranges_coverage_pct + 0.5 * approval_rate

        return DimensionAssessment(
            dimension=QualityDimension.ASSUMPTIONS,
            score=score,
            applicable=True,
            inputs_used={
                "ranges_coverage_pct": ranges_coverage_pct,
                "approval_rate": approval_rate,
            },
            rules_triggered=["assumptions_blend"],
        )

    # ---------------------------------------------------------------
    # Dimension 4: Constraints
    # ---------------------------------------------------------------

    def score_constraints(
        self,
        confidence_summary: dict[str, int] | None,
    ) -> DimensionAssessment:
        """Score constraint confidence from HARD/ESTIMATED/ASSUMED counts.

        Weighted average: HARD*1.0 + ESTIMATED*0.6 + ASSUMED*0.3.
        Returns not-applicable when confidence_summary is None or empty.
        """
        if confidence_summary is None or not confidence_summary:
            return DimensionAssessment(
                dimension=QualityDimension.CONSTRAINTS,
                score=0.0,
                applicable=False,
                inputs_used={"confidence_summary": confidence_summary},
                rules_triggered=["constraints_not_applicable"],
            )

        total = sum(confidence_summary.values())
        if total == 0:
            return DimensionAssessment(
                dimension=QualityDimension.CONSTRAINTS,
                score=0.0,
                applicable=False,
                inputs_used={"confidence_summary": confidence_summary},
                rules_triggered=["constraints_not_applicable"],
            )

        weighted_sum = sum(
            count * _CONSTRAINT_WEIGHTS.get(label, 0.0)
            for label, count in confidence_summary.items()
        )
        score = weighted_sum / total

        return DimensionAssessment(
            dimension=QualityDimension.CONSTRAINTS,
            score=score,
            applicable=True,
            inputs_used={"confidence_summary": confidence_summary},
            rules_triggered=["constraints_weighted_average"],
        )

    # ---------------------------------------------------------------
    # Dimension 5: Workforce
    # ---------------------------------------------------------------

    def score_workforce(
        self,
        overall_confidence: str | None,
    ) -> DimensionAssessment:
        """Score workforce data confidence.

        Maps HIGH->1.0, MEDIUM->0.6, LOW->0.3.
        Returns not-applicable when overall_confidence is None.
        """
        if overall_confidence is None:
            return DimensionAssessment(
                dimension=QualityDimension.WORKFORCE,
                score=0.0,
                applicable=False,
                inputs_used={"overall_confidence": overall_confidence},
                rules_triggered=["workforce_not_applicable"],
            )

        score = _WORKFORCE_SCORES.get(overall_confidence, 0.0)

        return DimensionAssessment(
            dimension=QualityDimension.WORKFORCE,
            score=score,
            applicable=True,
            inputs_used={"overall_confidence": overall_confidence},
            rules_triggered=[f"workforce_confidence_{overall_confidence.lower()}"],
        )

    # ---------------------------------------------------------------
    # Dimension 6: Plausibility
    # ---------------------------------------------------------------

    def score_plausibility(
        self,
        multipliers_in_range_pct: float,
        flagged_count: int,
    ) -> DimensionAssessment:
        """Score plausibility from multiplier range check results.

        Score = multipliers_in_range_pct / 100.0
        """
        score = multipliers_in_range_pct / 100.0

        return DimensionAssessment(
            dimension=QualityDimension.PLAUSIBILITY,
            score=score,
            applicable=True,
            inputs_used={
                "multipliers_in_range_pct": multipliers_in_range_pct,
                "flagged_count": flagged_count,
            },
            rules_triggered=["plausibility_range_check"],
        )

    # ---------------------------------------------------------------
    # Dimension 7: Freshness
    # ---------------------------------------------------------------

    def score_freshness(
        self,
        source_ages: list[SourceAge],
    ) -> DimensionAssessment:
        """Score data freshness across all time-scored sources.

        Filters out PER_ENGAGEMENT sources, computes age/cadence ratios,
        walks freshness_ratio_thresholds for each, then averages.
        """
        # Filter out PER_ENGAGEMENT sources.
        time_scored = [
            sa
            for sa in source_ages
            if sa.expected_frequency != SourceUpdateFrequency.PER_ENGAGEMENT
        ]

        if not time_scored:
            return DimensionAssessment(
                dimension=QualityDimension.FRESHNESS,
                score=0.0,
                applicable=False,
                inputs_used={"source_ratios": []},
                rules_triggered=["freshness_no_time_scored_sources"],
            )

        source_ratios: list[dict[str, object]] = []
        scores: list[float] = []

        for sa in time_scored:
            cadence_days = FREQUENCY_DAYS[sa.expected_frequency]
            ratio = sa.age_days / cadence_days

            # Walk thresholds to find the score for this ratio.
            source_score = 0.0
            rule = "freshness_fallback"
            for max_ratio, bracket_score in self._config.freshness_ratio_thresholds:
                if ratio <= max_ratio:
                    source_score = bracket_score
                    rule = f"freshness_ratio_le_{max_ratio}"
                    break

            scores.append(source_score)
            source_ratios.append(
                {
                    "source_name": sa.source_name,
                    "ratio": round(ratio, 4),
                    "score": source_score,
                    "rule": rule,
                }
            )

        avg_score = sum(scores) / len(scores)

        return DimensionAssessment(
            dimension=QualityDimension.FRESHNESS,
            score=avg_score,
            applicable=True,
            inputs_used={"source_ratios": source_ratios},
            rules_triggered=["freshness_ratio_average"],
        )

    # ---------------------------------------------------------------
    # Composite Score
    # ---------------------------------------------------------------

    # Grade ordering for completeness cap comparisons (best -> worst).
    _GRADE_ORDER: list[QualityGrade] = [
        QualityGrade.A,
        QualityGrade.B,
        QualityGrade.C,
        QualityGrade.D,
        QualityGrade.F,
    ]

    def composite_score(
        self,
        dimension_assessments: list[DimensionAssessment],
    ) -> RunQualityAssessment:
        """Compute a composite quality score from per-dimension assessments.

        1. Separate applicable vs not-applicable dimensions.
        2. Compute completeness_pct.
        3. Compute weighted average of applicable scores (normalized weights).
        4. Determine grade from config.grade_thresholds.
        5. Apply completeness cap (Amendment 1).
        6. Collect all warnings and count by severity.
        7. Return RunQualityAssessment.
        """
        total = len(dimension_assessments)

        # Separate applicable vs not-applicable.
        applicable = [da for da in dimension_assessments if da.applicable]
        not_applicable = [da for da in dimension_assessments if not da.applicable]

        # Completeness.
        completeness_pct = (len(applicable) / total * 100.0) if total > 0 else 0.0

        # Weighted average of applicable scores.
        if applicable:
            raw_weights = {
                da.dimension.value: self._config.dimension_weights.get(
                    da.dimension.value, 0.0
                )
                for da in applicable
            }
            weight_sum = sum(raw_weights.values())
            if weight_sum > 0:
                score = sum(
                    da.score * raw_weights[da.dimension.value] / weight_sum
                    for da in applicable
                )
            else:
                score = 0.0
        else:
            score = 0.0

        # Determine grade from thresholds.
        grade = QualityGrade.F
        for grade_letter in ("A", "B", "C", "D"):
            threshold = self._config.grade_thresholds[grade_letter]
            if score >= threshold:
                grade = QualityGrade(grade_letter)
                break

        # Completeness cap (Amendment 1).
        if completeness_pct < 30:
            cap = QualityGrade(self._config.completeness_cap_30)
        elif completeness_pct < 50:
            cap = QualityGrade(self._config.completeness_cap_50)
        else:
            cap = None

        if cap is not None:
            cap_idx = self._GRADE_ORDER.index(cap)
            grade_idx = self._GRADE_ORDER.index(grade)
            if grade_idx < cap_idx:
                # Grade is better than cap — clamp it.
                grade = cap

        # Collect all warnings from all dimension assessments.
        all_warnings: list[QualityWarning] = []
        for da in dimension_assessments:
            all_warnings.extend(da.warnings)

        # Count warnings by severity.
        waiver_required_count = sum(
            1 for w in all_warnings if w.severity == QualitySeverity.WAIVER_REQUIRED
        )
        critical_count = sum(
            1 for w in all_warnings if w.severity == QualitySeverity.CRITICAL
        )
        warning_count = sum(
            1 for w in all_warnings if w.severity == QualitySeverity.WARNING
        )
        info_count = sum(
            1 for w in all_warnings if w.severity == QualitySeverity.INFO
        )

        # Build dimension lists.
        assessed_dims = [da.dimension for da in dimension_assessments]
        applicable_dims = [da.dimension for da in applicable]
        all_dims = set(QualityDimension)
        missing_dims = sorted(
            all_dims - set(assessed_dims), key=lambda d: d.value
        )

        return RunQualityAssessment(
            assessment_version=1,
            dimension_assessments=dimension_assessments,
            applicable_dimensions=applicable_dims,
            assessed_dimensions=assessed_dims,
            missing_dimensions=missing_dims,
            completeness_pct=completeness_pct,
            composite_score=score,
            grade=grade,
            warnings=all_warnings,
            waiver_required_count=waiver_required_count,
            critical_count=critical_count,
            warning_count=warning_count,
            info_count=info_count,
        )
